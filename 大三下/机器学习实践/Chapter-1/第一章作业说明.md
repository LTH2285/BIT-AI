# 第一章作业说明

王中琦 1120190892

### Code Tree

|ML源码

​		|gradient_decent_01.ipynb # 二次函数梯度下降

​		|gradient_decent_02.ipynb # 求解线性回归

|mindspore版

​		|gradient_decent_01.py # 使用mindspore.numpy进行修改

​		|gradient_decent_02.py # 使用mindspore的Tensor进行修改

------

### gradient_decent_01.py

#### 代码说明

1. 功能：可视化梯度下降过程
2. mindspore化：使用mindspore.numpy进行修改，替换部分原有numpy类型改为mindspore的tensor类型

#### 函数说明

- **J**(theta)： ==$y=(x-2.5)^2-1$==
  - 输入：
    - theta：x坐标值
  - 输出：
    - y值
- **dJ**(theta)：==$dy=2(x-2.5)$==
  - 输入：
    - theta：x坐标值
  - 输出：
    - 求导结果
- **gradient_descent**(initial_theta, theta_history, n_iter = 1e4, epsilon=1e-8, eta=0.01)：==梯度下降==
  - 输入：
    - initial_theta：原始参数
    - theta_history：迭代中各参数记录（初值为空列表）
    - n_iter：总迭代次数
    - epsilon：阈值，更新与未更新差值若小于阈值则停止迭代
    - eta：学习率
  - 输出：
    - theta_history：迭代中各参数记录
- **plot_theta_history**(plot_x,theta_history)： ==梯度可视化==
  - 输入：
    - plot_x：x值
    - theta_history：迭代中各参数记录
  - 输出：None

#### 结果如下：

<img src="C:\Users\24857\AppData\Roaming\Typora\typora-user-images\image-20220405190710124.png" alt="image-20220405190710124" style="zoom: 80%;" />

------

### gradient_decent_02.py

#### 代码说明

1. 线性回归中，比较梯度下降和最小二乘解
2. mindspore化：使用mindspore的Tensor进行修改，除画图外中间变量类型为mindspore的tensor类型

#### 函数说明

- **ols_algebra**(x, y)：==最小二乘解，$W = (X^TX)^{-1}X^Ty$==
  - 输入：
    - x：一系列X值（Tensor类型）
    - y：一系列Y值（Tensor类型）
  - 输出：
    - w1：斜率
    - w2：截距
- **ols_gradient_descent**(x,y,lr,num_iter)： ==梯度下降解==
  - 输入：
    - x：一系列X值（Tensor类型）
    - y：一系列Y值（Tensor类型）
    - lr：学习率
    - num_iter：最大训练轮数
  - 输出：
    - w1：斜率
    - w2：截距

#### 结果如下：

<img src="C:\Users\24857\Desktop\output2.png" alt="output2" style="zoom:80%;" />